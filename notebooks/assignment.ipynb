{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6dbc95",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a1d6d",
   "metadata": {},
   "source": [
    "**Goal**: The goal of this assignment is to use the packages presented before, i.e. pydantic, pandera and optuna, in a real case scenario, where you want to train a model and find the best hyperparameters.\n",
    "\n",
    "Using an open source insurance dataset, we will do the different steps:\n",
    "* Validate the dataset, making sure it is prepared to train a model using pandera.\n",
    "* Validate the input parameters for the bayesian optimization using pydantic.\n",
    "\n",
    "**Table of contents**:\n",
    "1. [Validate the dataset using pandera](#1-validate-the-dataset-using-pandera)\n",
    "2. [Validate the parameters using pydantic](#2-validate-the-parameters-using-pydantic)\n",
    "3. [Bonus](#3-bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501acb0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53bebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))  # adjust .parent depth so 'src' is findable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1e90c",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import pandera.pandas as pa\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from pydantic import (\n",
    "    BaseModel,\n",
    "    Field,\n",
    "    StrictStr,\n",
    "    field_validator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_utils import (\n",
    "    load_conf_parameters,\n",
    "    retrieve_data_w_features,\n",
    "    run_bayesian_optimization,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d16455",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e432369",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/01_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b28107",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b9912",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"{DATA_PATH}/fremotor1prem0304.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a677555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e9022",
   "metadata": {},
   "source": [
    "# 1. Validate the dataset using pandera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d6d65",
   "metadata": {},
   "source": [
    "**Goal**: The goal of this section is to validate the dataframe we will use to train our model.\n",
    "\n",
    "We want to validate the dataframe before training our model. Your goal is to make sure the columns will verify the following rules:\n",
    "* **Year**: Check the year are between 2003 and 2004.\n",
    "* **DrivAge**: Make sure the driver's age are possible (e.g. between 18 and 100).\n",
    "* **DrivGender**: The gender is either 'M' or 'F'.\n",
    "* **MaritalStatus**: Possible values \"Cohabiting\", \"Married\", \"Single\", \"Widowed\" or \"Divorced\".\n",
    "* **BonusMalus**: The value of the bonus / malus is over 50.\n",
    "* **LicenceNb**: The licence number is over 1.\n",
    "* **JobCode**: The possible values are \"Private employee\", \"Public employee\", \"Retiree\", \"Other\", \"Craftsman\", \"Farmer\" or \"Retailer\",\n",
    "* **VehAge**: Make sure the vehicule age is possible.\n",
    "* **VehGas**: Either \"Regular\" or \"Diesel\".\n",
    "* **Area**: Possible values are from A1 to A12 included.\n",
    "\n",
    "**Exercise**: Update the schema to check the rules defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_df = pa.DataFrameSchema({\n",
    "    \"IDpol\": pa.Column(str),\n",
    "    \"PayFreq\": pa.Column(str, checks=pa.Check.isin([\"Annual\", \"Half-yearly\", \"Quarterly\", \"Monthly\"])),\n",
    "    \"VehClass\": pa.Column(str, checks=pa.Check.isin([\n",
    "        \"Cheapest\", \"Cheaper\", \"Cheap\", \"Medium low\", \"Medium\", \"Medium high\", \"Expensive\", \"More expensive\", \"Most expensive\",\n",
    "    ])),\n",
    "    \"VehPower\": pa.Column(str, checks=pa.Check.isin([f\"P{i}\" for i in range(1, 21)])),\n",
    "    \"VehUsage\": pa.Column(str, checks=pa.Check.isin([\n",
    "        \"Private+trip to office\", \"Professional\", \"Professional run\",\n",
    "    ])),\n",
    "    \"Garage\": pa.Column(str, checks=pa.Check.isin([\n",
    "        \"Closed zbox\", \"Closed collective parking\", \"Opened collective parking\", \"Street\",\n",
    "    ])),\n",
    "    ######################\n",
    "    ### YOUR CODE HERE ###\n",
    "    ######################\n",
    "})\n",
    "\n",
    "df_validated = schema_df.validate(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c51faf",
   "metadata": {},
   "source": [
    "However, as you can see, some features contain NaN values. To handle this issue, you have several solutions:\n",
    "* In the dataframe schema from pandera, set the option of possible NaN values to True. We don't recommend this approach as many models can't handle NaN values or you want to make sure to use them properly.\n",
    "* You can set the option in the pandera schema to drop rows with NaN values.\n",
    "* You can handle it with the classical feature engineering technics (feature imputing, creating new category, etc.)\n",
    "\n",
    "**Exercise**: Create a function to handle missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c7f3d",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click to reveal tip</summary>\n",
    "\n",
    "Use the `mode()[0]` function to get the most represented category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### YOUR CODE HERE ###\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe3242",
   "metadata": {},
   "source": [
    "To train the model, splitting the dataset between train, validation and test sets is important. You want to make sure each row is splitted into one and only one of the sets. To do so, you can use custom functions from pandera.\n",
    "\n",
    "**Exercise**: Adapt the schema to check that each row is in one and only one set.\n",
    "1. Add the right checks for the columns `train_set`, `val_set` and `test_set`.\n",
    "2. Add a schema check to make sure that each row is assigned to one and only one set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6d250",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click to reveal tip</summary>\n",
    "\n",
    "Use the `checks` parameter of the schema directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### YOUR CODE HERE ###\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebf4de",
   "metadata": {},
   "source": [
    "We also have the `big_train_set` column which indicates rows contained in the train or validation sets.\n",
    "\n",
    "**Exercise**: Adapt the schema to check that rows in either train or validation sets are in the big train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ab46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### YOUR CODE HERE ###\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9ebf9",
   "metadata": {},
   "source": [
    "# 2. Validate the parameters using pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbf6cc",
   "metadata": {},
   "source": [
    "**Goal**: The goal of this section is to display a real use of the pydantic validations to check that the parameters passed to your code are ones you accept.\n",
    "The validations will be applied to parameters to use for the optuna modelisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f160f15",
   "metadata": {},
   "source": [
    "**Exercise**: Create a pydantic class to validate the parameters given in a conf file to make sure they respect some rules to catch bugs before running your code:\n",
    "1. Define the attribute `search_space` which represents the search space for the Bayesian optimization.\n",
    "2. Define the attribute `categorical_feat` which represents the categorical features.\n",
    "3. Define the attribute `default_params` which represents the default combination of hyperparameters to test at the beginning of the Bayesian optimization.\n",
    "4. Create a custom validator to validate the `search_space`. The search space should be a dict where the keys are parameters names and the values are dict containing:\n",
    "   * `sampling_type`: The type of the parameter. Should be `categorical`, `int` or `float`.\n",
    "   * If the sampling type is categorical, an element `choices` containing the possible values.\n",
    "   * If the sampling type is either int or float, elements `min` and `max` containing the minimum and maximum values for the search space.\n",
    "\n",
    "For example:\n",
    "```yaml\n",
    "search_space: {\n",
    "    \"param_1\": {\n",
    "        \"sampling_type\": \"categorical\",\n",
    "        \"choices\": [\"cat_1\", \"cat_2\"],\n",
    "    },\n",
    "    \"param_2\":{\n",
    "        \"sampling_type\": \"int\",\n",
    "        \"min\": min_val,\n",
    "        \"max\": max_val,\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class validate_input_parameters(BaseModel):\n",
    "    \"\"\"A pydantic class to validate the input parameters of the process.\"\"\"\n",
    "    target_name: StrictStr = Field(\n",
    "        pattern=r\"^[A-Za-z0-1\\_]+$\",\n",
    "        description=\"Name of the target column.\",\n",
    "        frozen=True,\n",
    "    )\n",
    "    # ========================\n",
    "    # ==== YOUR CODE HERE ====\n",
    "    # ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b77c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_conf_parameters(\"../conf/parameters.yml\")\n",
    "\n",
    "validated_params = validate_input_parameters(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b237f20",
   "metadata": {},
   "source": [
    "As you can see, some parameters don't follow the rules you defined in the pydantic class!\n",
    "\n",
    "**Exercise**: Fix the parameters in the conf file to follow rules you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "### ACTIONS TO DO ###\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84957e3a",
   "metadata": {},
   "source": [
    "Then, we can run the Bayesian optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e19815",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"IDpol\", \"Year\", \"train_set\", \"val_set\", \"test_set\", \"big_train_set\"]\n",
    "\n",
    "X_big_train, y_big_train = retrieve_data_w_features(df=df, features_to_drop=cols_to_drop, split=\"big_train_set\")\n",
    "X_train, y_train = retrieve_data_w_features(df=df, features_to_drop=cols_to_drop, split=\"train_set\")\n",
    "X_val, y_val = retrieve_data_w_features(df=df, features_to_drop=cols_to_drop, split=\"val_set\")\n",
    "X_test, y_test = retrieve_data_w_features(df=df, features_to_drop=cols_to_drop, split=\"test_set\")\n",
    "\n",
    "best_params = run_bayesian_optimization(\n",
    "    df_train=X_train,\n",
    "    y_train=y_train,\n",
    "    df_val=X_val,\n",
    "    y_val=y_val,\n",
    "    categorical_features=validated_params.categorical_feat,\n",
    "    search_params=validated_params.search_space,\n",
    "    default_params_list=validated_params.default_params,\n",
    ")\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2c719",
   "metadata": {},
   "source": [
    "Now that we have found the best hyperparameters, we can train on the big train set and evaluate on the test set!\n",
    "\n",
    "**Exercise**: Train a final model on the big train set using the best hyperparameters and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3439324",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### YOUR CODE HERE ###\n",
    "######################\n",
    "\n",
    "big_train_predictions = #\n",
    "test_predictions = #\n",
    "\n",
    "big_train_rmse = root_mean_squared_error(y_true=y_big_train, y_pred=big_train_predictions)\n",
    "print(f\"Big Train RMSE: {big_train_rmse}\")\n",
    "test_rmse = root_mean_squared_error(y_true=y_test, y_pred=test_predictions)\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ea444",
   "metadata": {},
   "source": [
    "# 3. Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db4fca",
   "metadata": {},
   "source": [
    "You can see that there is overfitting! \n",
    "\n",
    "**Exercise**: Modify the function in the `train_utils.py` file to limit the overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879aea2",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click to reveal tip 1</summary>\n",
    "\n",
    "Add an element in the metric to optimize containing the delta between train and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ac360",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click to reveal tip 2</summary>\n",
    "\n",
    "Use an alpha parameter to control the impact of this delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### YOUR CODE HERE ###\n",
    "######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
